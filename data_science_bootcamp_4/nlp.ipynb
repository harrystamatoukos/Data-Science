{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text processing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from nltk.stem.snowball import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Harry\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Goal: build a model that will predict whether some review is 5 stars or 1 star"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'yelp.csv'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'yelp.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read yelp reviews into a DataFrame\n",
    "yelp = pd.read_csv('data/yelp.csv')\n",
    "\n",
    "#Create a dataframe that only contains 5 star and 1 star reviews\n",
    "best_worst = yelp[(yelp.stars == 5)| (yelp.stars ==1)]\n",
    "\n",
    "#Define X and y\n",
    "X = best_worst.text\n",
    "y = best_worst.stars\n",
    "\n",
    "#Split in training and testing sets\n",
    "X_train, X_test, y_train, y_test  = train_test_split(X, y , random_state = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Features with Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use Countvectorizer to create a document-term matrix from X_train and X_test\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vect = CountVectorizer()\n",
    "\n",
    "X_train_dtm = vect.fit_transform(X_train)\n",
    "X_test_dtm = vect.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['00',\n",
       " '000',\n",
       " '00a',\n",
       " '00am',\n",
       " '00pm',\n",
       " '01',\n",
       " '02',\n",
       " '03',\n",
       " '03342',\n",
       " '04',\n",
       " '05',\n",
       " '06',\n",
       " '07',\n",
       " '09',\n",
       " '0buxoc0crqjpvkezo3bqog',\n",
       " '0l',\n",
       " '10',\n",
       " '100',\n",
       " '1000',\n",
       " '1000x',\n",
       " '1001',\n",
       " '100th',\n",
       " '101',\n",
       " '102',\n",
       " '105',\n",
       " '1070',\n",
       " '108',\n",
       " '10am',\n",
       " '10ish',\n",
       " '10min',\n",
       " '10mins',\n",
       " '10minutes',\n",
       " '10pm',\n",
       " '10th',\n",
       " '10x',\n",
       " '11',\n",
       " '110',\n",
       " '1100',\n",
       " '111',\n",
       " '111th',\n",
       " '112',\n",
       " '115th',\n",
       " '118',\n",
       " '11a',\n",
       " '11am',\n",
       " '11p',\n",
       " '11pm',\n",
       " '12',\n",
       " '120',\n",
       " '128i',\n",
       " '129',\n",
       " '12am',\n",
       " '12oz',\n",
       " '12pm',\n",
       " '12th',\n",
       " '13',\n",
       " '14',\n",
       " '140',\n",
       " '147',\n",
       " '14lbs',\n",
       " '15',\n",
       " '150',\n",
       " '1500',\n",
       " '150mm',\n",
       " '15am',\n",
       " '15mins',\n",
       " '15pm',\n",
       " '15th',\n",
       " '16',\n",
       " '160',\n",
       " '165',\n",
       " '169',\n",
       " '16th',\n",
       " '17',\n",
       " '17p',\n",
       " '18',\n",
       " '180',\n",
       " '18th',\n",
       " '19',\n",
       " '1900',\n",
       " '1913',\n",
       " '1928',\n",
       " '1929',\n",
       " '1930s',\n",
       " '1940',\n",
       " '1952',\n",
       " '1955',\n",
       " '1956',\n",
       " '1960',\n",
       " '1961',\n",
       " '1969',\n",
       " '1970',\n",
       " '1980',\n",
       " '1980s',\n",
       " '1987',\n",
       " '1990s',\n",
       " '1992',\n",
       " '1995',\n",
       " '1996',\n",
       " '1998',\n",
       " '1999',\n",
       " '19th',\n",
       " '1cent',\n",
       " '1k',\n",
       " '1p',\n",
       " '1pm',\n",
       " '1st',\n",
       " '20',\n",
       " '200',\n",
       " '2002',\n",
       " '2003',\n",
       " '2004',\n",
       " '2005',\n",
       " '2006',\n",
       " '2007',\n",
       " '2008',\n",
       " '2009',\n",
       " '200lbs',\n",
       " '2010',\n",
       " '2011',\n",
       " '2012',\n",
       " '202',\n",
       " '20mbs',\n",
       " '20miles',\n",
       " '20min',\n",
       " '20pm',\n",
       " '20s',\n",
       " '20th',\n",
       " '20x',\n",
       " '21',\n",
       " '22',\n",
       " '220',\n",
       " '2240',\n",
       " '22oz',\n",
       " '23',\n",
       " '24',\n",
       " '24hrs',\n",
       " '24st',\n",
       " '24th',\n",
       " '25',\n",
       " '250',\n",
       " '25b',\n",
       " '25min',\n",
       " '25th',\n",
       " '26',\n",
       " '260',\n",
       " '2600',\n",
       " '2608',\n",
       " '2669',\n",
       " '26th',\n",
       " '27',\n",
       " '272',\n",
       " '28',\n",
       " '29',\n",
       " '29th',\n",
       " '2am',\n",
       " '2mbps',\n",
       " '2nd',\n",
       " '2pm',\n",
       " '2rd',\n",
       " '2wice',\n",
       " '2x',\n",
       " '30',\n",
       " '300',\n",
       " '3000',\n",
       " '30am',\n",
       " '30p',\n",
       " '30pm',\n",
       " '30th',\n",
       " '31',\n",
       " '311',\n",
       " '312',\n",
       " '32',\n",
       " '33',\n",
       " '33rd',\n",
       " '34',\n",
       " '34th',\n",
       " '35',\n",
       " '350ib',\n",
       " '35c',\n",
       " '35th',\n",
       " '36',\n",
       " '37',\n",
       " '370',\n",
       " '38',\n",
       " '38th',\n",
       " '39',\n",
       " '3am',\n",
       " '3d',\n",
       " '3g',\n",
       " '3k',\n",
       " '3lbs',\n",
       " '3n9u549zse8up',\n",
       " '3p',\n",
       " '3pm',\n",
       " '3rd',\n",
       " '3x',\n",
       " '40',\n",
       " '400',\n",
       " '4000',\n",
       " '40lm',\n",
       " '40min',\n",
       " '40th',\n",
       " '41',\n",
       " '411',\n",
       " '4113416766',\n",
       " '42',\n",
       " '420',\n",
       " '43',\n",
       " '44',\n",
       " '4458',\n",
       " '44th',\n",
       " '45',\n",
       " '453990',\n",
       " '45min',\n",
       " '45pm',\n",
       " '46',\n",
       " '475',\n",
       " '48',\n",
       " '480',\n",
       " '48th',\n",
       " '49',\n",
       " '490',\n",
       " '4b',\n",
       " '4hr',\n",
       " '4pm',\n",
       " '4th',\n",
       " '50',\n",
       " '500',\n",
       " '50cents',\n",
       " '50lm',\n",
       " '50s',\n",
       " '51',\n",
       " '51pm',\n",
       " '52',\n",
       " '5231',\n",
       " '53',\n",
       " '53pm',\n",
       " '54',\n",
       " '55',\n",
       " '56',\n",
       " '57',\n",
       " '59',\n",
       " '59th',\n",
       " '5k',\n",
       " '5min',\n",
       " '5p',\n",
       " '5pm',\n",
       " '5stars',\n",
       " '5th',\n",
       " '60',\n",
       " '600',\n",
       " '602',\n",
       " '61',\n",
       " '61st',\n",
       " '62010',\n",
       " '623',\n",
       " '63',\n",
       " '64',\n",
       " '64th',\n",
       " '65',\n",
       " '66',\n",
       " '67',\n",
       " '68',\n",
       " '680',\n",
       " '69',\n",
       " '6am',\n",
       " '6p',\n",
       " '6pm',\n",
       " '6th',\n",
       " '70',\n",
       " '700',\n",
       " '70s',\n",
       " '70th',\n",
       " '71',\n",
       " '71st',\n",
       " '75',\n",
       " '750',\n",
       " '755891987',\n",
       " '76',\n",
       " '79',\n",
       " '7am',\n",
       " '7pm',\n",
       " '7th',\n",
       " '80',\n",
       " '800',\n",
       " '8000hp',\n",
       " '80s',\n",
       " '81',\n",
       " '83',\n",
       " '832',\n",
       " '83rd',\n",
       " '85',\n",
       " '85154658',\n",
       " '86',\n",
       " '88',\n",
       " '89',\n",
       " '8am',\n",
       " '8pm',\n",
       " '8th',\n",
       " '8v',\n",
       " '8yo',\n",
       " '90',\n",
       " '90s',\n",
       " '91',\n",
       " '911',\n",
       " '945am',\n",
       " '95',\n",
       " '96',\n",
       " '977',\n",
       " '98',\n",
       " '99',\n",
       " '9999',\n",
       " '99cent',\n",
       " '9oz',\n",
       " '9p',\n",
       " '9pm',\n",
       " '9year',\n",
       " '9yo',\n",
       " '______',\n",
       " '_______________',\n",
       " '_c',\n",
       " '_gyib8ea4hdfylss17zc_g',\n",
       " 'a1',\n",
       " 'aa',\n",
       " 'aaa',\n",
       " 'aaaamazing',\n",
       " 'aaammmazzing',\n",
       " 'aaron',\n",
       " 'ab',\n",
       " 'abandoned',\n",
       " 'abandoning',\n",
       " 'abba',\n",
       " 'abbreviate',\n",
       " 'abbreviated',\n",
       " 'abby',\n",
       " 'abc',\n",
       " 'abdomen',\n",
       " 'abilities',\n",
       " 'ability',\n",
       " 'able',\n",
       " 'abodoba',\n",
       " 'abound',\n",
       " 'about',\n",
       " 'above',\n",
       " 'abrasion',\n",
       " 'abroad',\n",
       " 'abrupt',\n",
       " 'absent',\n",
       " 'absinthe',\n",
       " 'absoloutely',\n",
       " 'absolut',\n",
       " 'absolute',\n",
       " 'absolutely',\n",
       " 'absolutley',\n",
       " 'absolutly',\n",
       " 'abstained',\n",
       " 'absurd',\n",
       " 'abuelo',\n",
       " 'abundance',\n",
       " 'abundant',\n",
       " 'abuse',\n",
       " 'abused',\n",
       " 'abusive',\n",
       " 'ac',\n",
       " 'academy',\n",
       " 'acapulco',\n",
       " 'accent',\n",
       " 'accented',\n",
       " 'accents',\n",
       " 'accept',\n",
       " 'acceptable',\n",
       " 'accepted',\n",
       " 'access',\n",
       " 'accessed',\n",
       " 'accessible',\n",
       " 'accessories',\n",
       " 'accessorize',\n",
       " 'accessory',\n",
       " 'accident',\n",
       " 'accidental',\n",
       " 'accidentally',\n",
       " 'accolades',\n",
       " 'accommodate',\n",
       " 'accommodated',\n",
       " 'accommodates',\n",
       " 'accommodating',\n",
       " 'accommodations',\n",
       " 'accomodate',\n",
       " 'accomodating',\n",
       " 'accompanied',\n",
       " 'accompanies',\n",
       " 'accompaniment',\n",
       " 'accompany',\n",
       " 'accompanying',\n",
       " 'accomplish',\n",
       " 'accomplished',\n",
       " 'accomplishment',\n",
       " 'according',\n",
       " 'accordingly',\n",
       " 'account',\n",
       " 'accounts',\n",
       " 'accoutrement',\n",
       " 'accredited',\n",
       " 'accross',\n",
       " 'accurate',\n",
       " 'accurately',\n",
       " 'accusation',\n",
       " 'accustom',\n",
       " 'accustomed',\n",
       " 'accutemp',\n",
       " 'ace',\n",
       " 'ache',\n",
       " 'aches',\n",
       " 'achieve',\n",
       " 'achievement',\n",
       " 'acid',\n",
       " 'acidic',\n",
       " 'acknowledge',\n",
       " 'acknowledged',\n",
       " 'acknowledgement',\n",
       " 'acknowledging',\n",
       " 'ackward',\n",
       " 'acne',\n",
       " 'acoustic',\n",
       " 'acoustics',\n",
       " 'acquaintance',\n",
       " 'acres',\n",
       " 'acrid',\n",
       " 'acrimonious',\n",
       " 'across',\n",
       " 'acrylics',\n",
       " 'act',\n",
       " 'acted',\n",
       " 'acting',\n",
       " 'action',\n",
       " 'actions',\n",
       " 'activation',\n",
       " 'active',\n",
       " 'actively',\n",
       " 'activism',\n",
       " 'activities',\n",
       " 'activity',\n",
       " 'actor',\n",
       " 'actors',\n",
       " 'acts',\n",
       " 'actual',\n",
       " 'actually',\n",
       " 'actualy',\n",
       " 'actully',\n",
       " 'acute',\n",
       " 'acy',\n",
       " 'ad',\n",
       " 'adage',\n",
       " 'adam',\n",
       " 'adamant',\n",
       " 'adams',\n",
       " 'adapter',\n",
       " 'add',\n",
       " 'added',\n",
       " 'addendum',\n",
       " 'addict',\n",
       " 'addicted',\n",
       " 'addicting',\n",
       " 'addictingly',\n",
       " 'addiction',\n",
       " 'addictionovercome',\n",
       " 'addictive',\n",
       " 'addicts',\n",
       " 'adding',\n",
       " 'addition',\n",
       " 'additional',\n",
       " 'additionally',\n",
       " 'additions',\n",
       " 'address',\n",
       " 'addressed',\n",
       " 'addressing',\n",
       " 'adds',\n",
       " 'ade',\n",
       " 'adelman',\n",
       " 'adequate',\n",
       " 'adequately',\n",
       " 'adios',\n",
       " 'adjacent',\n",
       " 'adjoining',\n",
       " 'adjust',\n",
       " 'adjusted',\n",
       " 'adjustment',\n",
       " 'adjustments',\n",
       " 'administrative',\n",
       " 'admire',\n",
       " 'admission',\n",
       " 'admit',\n",
       " 'admits',\n",
       " 'admitted',\n",
       " 'admittedly',\n",
       " 'admitting',\n",
       " 'admonishment',\n",
       " 'adobada',\n",
       " 'adobe',\n",
       " 'adobo',\n",
       " 'adolescence',\n",
       " 'adopt',\n",
       " 'adopted',\n",
       " 'adoption',\n",
       " 'adoptions',\n",
       " 'adorable',\n",
       " 'adore',\n",
       " 'adorning',\n",
       " 'adovada',\n",
       " 'adquate',\n",
       " 'adrienne',\n",
       " 'ads',\n",
       " 'adult',\n",
       " 'adulthood',\n",
       " 'adults',\n",
       " 'advance',\n",
       " 'advanced',\n",
       " 'advancing',\n",
       " 'advantage',\n",
       " 'advantages',\n",
       " 'adventure',\n",
       " 'adventures',\n",
       " 'adventurous',\n",
       " 'adverse',\n",
       " 'adversity',\n",
       " 'advertise',\n",
       " 'advertised',\n",
       " 'advertisement',\n",
       " 'advertising',\n",
       " 'adverts',\n",
       " 'advice',\n",
       " 'advise',\n",
       " 'advised',\n",
       " 'advising',\n",
       " 'advisor',\n",
       " 'advocate',\n",
       " 'advocated',\n",
       " 'aeg',\n",
       " 'aerators',\n",
       " 'aerobic',\n",
       " 'aerobics',\n",
       " 'aeropress',\n",
       " 'aesthetically',\n",
       " 'aesthetics',\n",
       " 'affair',\n",
       " 'affect',\n",
       " 'affected',\n",
       " 'affects',\n",
       " 'afficianados',\n",
       " 'affiliated',\n",
       " 'affiliation',\n",
       " 'affluent',\n",
       " 'afforadable',\n",
       " 'afford',\n",
       " 'affordable',\n",
       " 'aficionado',\n",
       " 'aficionados',\n",
       " 'afloat',\n",
       " 'aforementioned',\n",
       " 'afoul',\n",
       " 'afraid',\n",
       " 'africa',\n",
       " 'after',\n",
       " 'afterall',\n",
       " 'afterdark',\n",
       " 'afterglow',\n",
       " 'afternoon',\n",
       " 'afternoons',\n",
       " 'aftertaste',\n",
       " 'afterthought',\n",
       " 'afterward',\n",
       " 'afterwards',\n",
       " 'again',\n",
       " 'against',\n",
       " 'agape',\n",
       " 'agave',\n",
       " 'agaves',\n",
       " 'age',\n",
       " 'aged',\n",
       " 'agency',\n",
       " 'agent',\n",
       " 'agents',\n",
       " 'ages',\n",
       " 'aggressive',\n",
       " 'aging',\n",
       " 'agitated',\n",
       " 'ago',\n",
       " 'agonizing',\n",
       " 'agree',\n",
       " 'agreeable',\n",
       " 'agreed',\n",
       " 'agreeing',\n",
       " 'agreement',\n",
       " 'agrees',\n",
       " 'agua',\n",
       " 'aguas',\n",
       " 'agwa',\n",
       " 'ah',\n",
       " 'ahead',\n",
       " 'ahem',\n",
       " 'ahh',\n",
       " 'ahhh',\n",
       " 'ahhhh',\n",
       " 'ahhhhh',\n",
       " 'ahi',\n",
       " 'ahold',\n",
       " 'ahwatukee',\n",
       " 'aid',\n",
       " 'aiello',\n",
       " 'aiko',\n",
       " 'aim',\n",
       " 'aimed',\n",
       " 'aimlessly',\n",
       " 'aims',\n",
       " 'ain',\n",
       " 'aint',\n",
       " 'aioli',\n",
       " 'aiptasia',\n",
       " 'air',\n",
       " 'airconditioned',\n",
       " 'airfair',\n",
       " 'airfare',\n",
       " 'airline',\n",
       " 'airlines',\n",
       " 'airpark',\n",
       " 'airplane',\n",
       " 'airport',\n",
       " 'airports',\n",
       " 'airwarys',\n",
       " 'airways',\n",
       " 'airy',\n",
       " 'aisha',\n",
       " 'aisle',\n",
       " 'aisles',\n",
       " 'aj',\n",
       " 'aji',\n",
       " 'ajo',\n",
       " 'ajos',\n",
       " 'ajs',\n",
       " 'ajvar',\n",
       " 'aka',\n",
       " 'aki',\n",
       " 'aknowledging',\n",
       " 'akor',\n",
       " 'al',\n",
       " 'ala',\n",
       " 'alabama',\n",
       " 'alain',\n",
       " 'alameda',\n",
       " 'alan',\n",
       " 'alarmed',\n",
       " 'alas',\n",
       " 'alaskan',\n",
       " 'alaus',\n",
       " 'albacore',\n",
       " 'albeit',\n",
       " 'alber',\n",
       " 'album',\n",
       " 'albums',\n",
       " 'alcohol',\n",
       " 'alcoholic',\n",
       " 'aldo',\n",
       " 'ale',\n",
       " 'alert',\n",
       " 'alex',\n",
       " 'alexandra',\n",
       " 'alfalfa',\n",
       " 'alfonso',\n",
       " 'alfred',\n",
       " 'alfredo',\n",
       " 'algae',\n",
       " 'ali',\n",
       " 'alice',\n",
       " 'alicia',\n",
       " 'aliens',\n",
       " 'alike',\n",
       " 'alittle',\n",
       " 'alive',\n",
       " 'all',\n",
       " 'alla',\n",
       " 'allayed',\n",
       " 'alleged',\n",
       " 'allegiant',\n",
       " 'allen',\n",
       " 'allende',\n",
       " 'allergen',\n",
       " 'allergic',\n",
       " 'allergies',\n",
       " 'allergy',\n",
       " 'alleviated',\n",
       " 'alley',\n",
       " 'alligator',\n",
       " 'allocating',\n",
       " 'allot',\n",
       " 'alloted',\n",
       " 'allotted',\n",
       " 'allow',\n",
       " 'allowable',\n",
       " 'allowance',\n",
       " 'allowed',\n",
       " 'allowing',\n",
       " 'allows',\n",
       " 'allure',\n",
       " 'almond',\n",
       " 'almonds',\n",
       " 'almost',\n",
       " 'aloe',\n",
       " 'alofts',\n",
       " 'aloha',\n",
       " 'alone',\n",
       " 'along',\n",
       " 'alongside',\n",
       " 'alons',\n",
       " 'aloo',\n",
       " 'aloof',\n",
       " 'alot',\n",
       " 'aloud',\n",
       " 'alp',\n",
       " 'already',\n",
       " 'alright',\n",
       " 'also',\n",
       " 'alteration',\n",
       " 'alterations',\n",
       " 'altercation',\n",
       " 'altered',\n",
       " 'altering',\n",
       " 'alternately',\n",
       " 'alternative',\n",
       " 'alternatively',\n",
       " 'alternatives',\n",
       " 'although',\n",
       " 'altic',\n",
       " 'aluminum',\n",
       " 'alway',\n",
       " 'always',\n",
       " 'am',\n",
       " 'ama',\n",
       " 'amaaaaazing',\n",
       " 'amaazing',\n",
       " 'amados',\n",
       " 'amalfi',\n",
       " 'amanda',\n",
       " 'amaretti',\n",
       " 'amaretto',\n",
       " 'amarillo',\n",
       " 'amaro',\n",
       " 'amateur',\n",
       " 'amaze',\n",
       " 'amazed',\n",
       " 'amazement',\n",
       " 'amazes',\n",
       " 'amazing',\n",
       " 'amazingly',\n",
       " 'amazingness',\n",
       " 'amazon',\n",
       " 'amazzzzzzing',\n",
       " 'ambassador',\n",
       " 'amber',\n",
       " 'ambiance',\n",
       " 'ambience',\n",
       " 'ambient',\n",
       " 'ambrosia',\n",
       " 'amc',\n",
       " 'amenable',\n",
       " 'amendment',\n",
       " 'amenities',\n",
       " 'america',\n",
       " 'american',\n",
       " 'americana',\n",
       " 'americanized',\n",
       " 'americano',\n",
       " 'americanos',\n",
       " 'americans',\n",
       " 'ami',\n",
       " 'amicable',\n",
       " 'amidst',\n",
       " 'amin',\n",
       " 'amish',\n",
       " 'ammo',\n",
       " 'amomi',\n",
       " 'among',\n",
       " 'amongst',\n",
       " 'amore',\n",
       " 'amount',\n",
       " 'amounted',\n",
       " 'amounts',\n",
       " 'amp',\n",
       " 'amphitheatre',\n",
       " 'ample',\n",
       " 'ampm',\n",
       " 'amtrack',\n",
       " 'amuse',\n",
       " 'amused',\n",
       " 'amusement',\n",
       " 'amusing',\n",
       " 'amy',\n",
       " 'an',\n",
       " 'anal',\n",
       " 'analysis',\n",
       " 'ancho',\n",
       " 'anchor',\n",
       " 'anchors',\n",
       " 'anchovies',\n",
       " 'anchovy',\n",
       " 'and',\n",
       " 'andiamo',\n",
       " 'andouille',\n",
       " 'andrea',\n",
       " 'andrew',\n",
       " 'andy',\n",
       " 'anecdotes',\n",
       " 'anemic',\n",
       " 'anesthetic',\n",
       " 'anew',\n",
       " 'angel',\n",
       " 'angela',\n",
       " 'angeles',\n",
       " 'angelic',\n",
       " 'angello',\n",
       " 'angels',\n",
       " 'anger',\n",
       " 'anglaise',\n",
       " 'angle',\n",
       " 'angler',\n",
       " 'angles',\n",
       " 'angry',\n",
       " 'angst',\n",
       " 'anibal',\n",
       " 'animal',\n",
       " 'animals',\n",
       " 'animated',\n",
       " 'animatronics',\n",
       " 'anise',\n",
       " 'aniston',\n",
       " 'ankle',\n",
       " 'ankles',\n",
       " 'ann',\n",
       " 'anne',\n",
       " 'annie',\n",
       " 'annihilator',\n",
       " 'anniversary',\n",
       " 'announced',\n",
       " 'annoy',\n",
       " 'annoyance',\n",
       " 'annoyed',\n",
       " 'annoying',\n",
       " 'annual',\n",
       " 'anomaly',\n",
       " 'anonymous',\n",
       " 'another',\n",
       " 'ans',\n",
       " 'ansel',\n",
       " 'answer',\n",
       " 'answered',\n",
       " 'answering',\n",
       " 'answers',\n",
       " 'ant',\n",
       " 'anthem',\n",
       " 'anthony',\n",
       " 'anti',\n",
       " 'anticipate',\n",
       " 'anticipated',\n",
       " 'anticipating',\n",
       " 'anticipation',\n",
       " 'antipasti',\n",
       " 'antipasto',\n",
       " 'antique',\n",
       " 'antiques',\n",
       " 'antiquing',\n",
       " 'antiseptic',\n",
       " 'antithesis',\n",
       " 'antler',\n",
       " 'antonio',\n",
       " 'antono',\n",
       " 'ants',\n",
       " 'anxiety',\n",
       " 'anxious',\n",
       " 'anxiously',\n",
       " 'any',\n",
       " 'anybody',\n",
       " 'anyday',\n",
       " 'anyhoo',\n",
       " 'anyhow',\n",
       " 'anymore',\n",
       " 'anyone',\n",
       " 'anyplace',\n",
       " 'anything',\n",
       " 'anythings',\n",
       " 'anytime',\n",
       " 'anyway',\n",
       " 'anyways',\n",
       " 'anywhere',\n",
       " 'anywho',\n",
       " 'ao',\n",
       " 'ap',\n",
       " 'apache',\n",
       " 'apart',\n",
       " 'apartment',\n",
       " 'apartments',\n",
       " 'ape',\n",
       " 'apiece',\n",
       " 'apologetic',\n",
       " 'apologetically',\n",
       " 'apologists',\n",
       " 'apologize',\n",
       " 'apologized',\n",
       " 'apologizes',\n",
       " 'apologizing',\n",
       " 'apology',\n",
       " 'apostles',\n",
       " 'apothecary',\n",
       " 'apothic',\n",
       " 'app',\n",
       " 'appalachians',\n",
       " 'appaled',\n",
       " 'appalled',\n",
       " 'appalling',\n",
       " 'apparantly',\n",
       " 'apparel',\n",
       " 'apparent',\n",
       " 'apparently',\n",
       " 'appauling',\n",
       " 'appeal',\n",
       " 'appealed',\n",
       " 'appealing',\n",
       " 'appeals',\n",
       " 'appear',\n",
       " 'appearance',\n",
       " 'appearances',\n",
       " 'appeared',\n",
       " 'appears',\n",
       " 'appeasing',\n",
       " 'appertizer',\n",
       " 'appetit',\n",
       " 'appetite',\n",
       " 'appetito',\n",
       " 'appetizaer',\n",
       " 'appetizer',\n",
       " 'appetizers',\n",
       " 'appetizing',\n",
       " 'applaud',\n",
       " 'apple',\n",
       " 'applebee',\n",
       " 'applebees',\n",
       " 'apples',\n",
       " 'appletini',\n",
       " 'appletinis',\n",
       " 'appliances',\n",
       " 'applied',\n",
       " 'applies',\n",
       " 'apply',\n",
       " 'applying',\n",
       " 'appoinmtnt',\n",
       " 'appointed',\n",
       " 'appointment',\n",
       " 'appointments',\n",
       " 'appologized',\n",
       " 'appraisal',\n",
       " 'appraisals',\n",
       " 'appraiser',\n",
       " 'appreciate',\n",
       " 'appreciated',\n",
       " 'appreciates',\n",
       " 'appreciation',\n",
       " 'appreciative',\n",
       " 'apprehensive',\n",
       " 'appreicate',\n",
       " 'approach',\n",
       " 'approachable',\n",
       " 'approached',\n",
       " 'approaches',\n",
       " 'approaching',\n",
       " 'appropriate',\n",
       " 'appropriately',\n",
       " 'approx',\n",
       " 'approximate',\n",
       " 'approximately',\n",
       " 'apps',\n",
       " 'appt',\n",
       " 'appys',\n",
       " 'apricot',\n",
       " 'april',\n",
       " 'apron',\n",
       " 'apt',\n",
       " 'aqua',\n",
       " 'aquarium',\n",
       " 'aquiring',\n",
       " 'ar',\n",
       " 'arabic',\n",
       " 'arai',\n",
       " 'arbol',\n",
       " 'arboreal',\n",
       " 'arborio',\n",
       " 'arcade',\n",
       " 'arcadia',\n",
       " 'arcane',\n",
       " 'arches',\n",
       " 'architect',\n",
       " 'architecture',\n",
       " ...]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check out the names of our features\n",
    "vect.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9393346379647749\n",
      "5    0.816691\n",
      "1    0.183309\n",
      "Name: stars, dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Harry\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n"
     ]
    }
   ],
   "source": [
    "# Intatntiate a model with sag solver\n",
    "lr = LogisticRegression(solver='sag')\n",
    "\n",
    "#Fit the model on the document-term matrix data\n",
    "lr.fit(X_train_dtm, y_train)\n",
    "\n",
    "#Create a list with predictions on testing data\n",
    "y_pred = lr.predict(X_test_dtm)\n",
    "\n",
    "#Print the accuracy score \n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "#Print baseline accuracy\n",
    "print(best_worst.stars.value_counts(normalize=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N-grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "N-grams are feauter which consist of N consecutive words\n",
    "```\n",
    "- Unigram :1-gram - 'my', 'cat','is','awesome'\n",
    "- Bigram : 2-gram -> 'my cat', 'cat is', 'is awesome'\n",
    "- Trigram: 3-gram -> 'my cat is', 'cat is awesome'\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Intantiate a vectorizer that include bigrams\n",
    "vect = CountVectorizer(ngram_range=(1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create  document-term matrices\n",
    "X_train_dtm = vect.fit_transform(X_train)\n",
    "X_test_dtm = vect.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9344422700587084\n"
     ]
    }
   ],
   "source": [
    "#Use Logistic Regression to predict star rating\n",
    "lr = LogisticRegression(solver='sag', max_iter = 10_000)\n",
    "\n",
    "#Fit the model on the document-term matrix data\n",
    "lr.fit(X_train_dtm, y_train)\n",
    "\n",
    "#Create a list with predictions on testing data\n",
    "y_pred = lr.predict(X_test_dtm)\n",
    "\n",
    "#Print the accuracy score \n",
    "print(metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stop words "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are Stop words :\n",
    "Why ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = CountVectorizer(stop_words ='english',ngram_range=(1,2), max_features=100_000, min_df = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create  document-term matrices\n",
    "X_train_dtm = vect.fit_transform(X_train)\n",
    "X_test_dtm = vect.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9315068493150684\n"
     ]
    }
   ],
   "source": [
    "#Use Logistic Regression to predict star rating\n",
    "lr = LogisticRegression(solver='sag', max_iter = 1000)\n",
    "\n",
    "#Fit the model on the document-term matrix data\n",
    "lr.fit(X_train_dtm, y_train)\n",
    "\n",
    "#Create a list with predictions on testing data\n",
    "y_pred = lr.predict(X_test_dtm)\n",
    "\n",
    "#Print the accuracy score \n",
    "print(metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming and Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conda install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Learn, Learner, Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Vectorizer \n",
    "Vectorize\n",
    "vector\n",
    "Vectoriser\n",
    "Vectorise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'My wife took me here on my birthday for breakfast and it was excellent.  The weather was perfect which made sitting outside overlooking their grounds an absolute pleasure.  Our waitress was excellent and our food arrived quickly on the semi-busy Saturday morning.  It looked like the place fills up pretty quickly so the earlier you get here the better.\\r\\n\\r\\nDo yourself a favor and get their Bloody Mary.  It was phenomenal and simply the best I\\'ve ever had.  I\\'m pretty sure they only use ingredients from their garden and blend them fresh when you order it.  It was amazing.\\r\\n\\r\\nWhile EVERYTHING on the menu looks excellent, I had the white truffle scrambled eggs vegetable skillet and it was tasty and delicious.  It came with 2 pieces of their griddled bread with was amazing and it absolutely made the meal complete.  It was the best \"toast\" I\\'ve ever had.\\r\\n\\r\\nAnyway, I can\\'t wait to go back!'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_worst.text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse a review through Textblob\n",
    "review = TextBlob(best_worst.text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextBlob(\"My wife took me here on my birthday for breakfast and it was excellent.  The weather was perfect which made sitting outside overlooking their grounds an absolute pleasure.  Our waitress was excellent and our food arrived quickly on the semi-busy Saturday morning.  It looked like the place fills up pretty quickly so the earlier you get here the better.\n",
       "\n",
       "Do yourself a favor and get their Bloody Mary.  It was phenomenal and simply the best I've ever had.  I'm pretty sure they only use ingredients from their garden and blend them fresh when you order it.  It was amazing.\n",
       "\n",
       "While EVERYTHING on the menu looks excellent, I had the white truffle scrambled eggs vegetable skillet and it was tasty and delicious.  It came with 2 pieces of their griddled bread with was amazing and it absolutely made the meal complete.  It was the best \"toast\" I've ever had.\n",
       "\n",
       "Anyway, I can't wait to go back!\")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordList(['My', 'wife', 'took', 'me', 'here', 'on', 'my', 'birthday', 'for', 'breakfast', 'and', 'it', 'was', 'excellent', 'The', 'weather', 'was', 'perfect', 'which', 'made', 'sitting', 'outside', 'overlooking', 'their', 'grounds', 'an', 'absolute', 'pleasure', 'Our', 'waitress', 'was', 'excellent', 'and', 'our', 'food', 'arrived', 'quickly', 'on', 'the', 'semi-busy', 'Saturday', 'morning', 'It', 'looked', 'like', 'the', 'place', 'fills', 'up', 'pretty', 'quickly', 'so', 'the', 'earlier', 'you', 'get', 'here', 'the', 'better', 'Do', 'yourself', 'a', 'favor', 'and', 'get', 'their', 'Bloody', 'Mary', 'It', 'was', 'phenomenal', 'and', 'simply', 'the', 'best', 'I', \"'ve\", 'ever', 'had', 'I', \"'m\", 'pretty', 'sure', 'they', 'only', 'use', 'ingredients', 'from', 'their', 'garden', 'and', 'blend', 'them', 'fresh', 'when', 'you', 'order', 'it', 'It', 'was', 'amazing', 'While', 'EVERYTHING', 'on', 'the', 'menu', 'looks', 'excellent', 'I', 'had', 'the', 'white', 'truffle', 'scrambled', 'eggs', 'vegetable', 'skillet', 'and', 'it', 'was', 'tasty', 'and', 'delicious', 'It', 'came', 'with', '2', 'pieces', 'of', 'their', 'griddled', 'bread', 'with', 'was', 'amazing', 'and', 'it', 'absolutely', 'made', 'the', 'meal', 'complete', 'It', 'was', 'the', 'best', 'toast', 'I', \"'ve\", 'ever', 'had', 'Anyway', 'I', 'ca', \"n't\", 'wait', 'to', 'go', 'back'])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review.words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['my', 'wife', 'took', 'me', 'here', 'on', 'my', 'birthday', 'for', 'breakfast', 'and', 'it', 'was', 'excel', 'the', 'weather', 'was', 'perfect', 'which', 'made', 'sit', 'outsid', 'overlook', 'their', 'ground', 'an', 'absolut', 'pleasur', 'our', 'waitress', 'was', 'excel', 'and', 'our', 'food', 'arriv', 'quick', 'on', 'the', 'semi-busi', 'saturday', 'morn', 'it', 'look', 'like', 'the', 'place', 'fill', 'up', 'pretti', 'quick', 'so', 'the', 'earlier', 'you', 'get', 'here', 'the', 'better', 'do', 'yourself', 'a', 'favor', 'and', 'get', 'their', 'bloodi', 'mari', 'it', 'was', 'phenomen', 'and', 'simpli', 'the', 'best', 'i', 've', 'ever', 'had', 'i', \"'m\", 'pretti', 'sure', 'they', 'onli', 'use', 'ingredi', 'from', 'their', 'garden', 'and', 'blend', 'them', 'fresh', 'when', 'you', 'order', 'it', 'it', 'was', 'amaz', 'while', 'everyth', 'on', 'the', 'menu', 'look', 'excel', 'i', 'had', 'the', 'white', 'truffl', 'scrambl', 'egg', 'veget', 'skillet', 'and', 'it', 'was', 'tasti', 'and', 'delici', 'it', 'came', 'with', '2', 'piec', 'of', 'their', 'griddl', 'bread', 'with', 'was', 'amaz', 'and', 'it', 'absolut', 'made', 'the', 'meal', 'complet', 'it', 'was', 'the', 'best', 'toast', 'i', 've', 'ever', 'had', 'anyway', 'i', 'ca', \"n't\", 'wait', 'to', 'go', 'back']\n"
     ]
    }
   ],
   "source": [
    "print([stemmer.stem(word) for word in review.words])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['My', 'wife', 'take', 'me', 'here', 'on', 'my', 'birthday', 'for', 'breakfast', 'and', 'it', 'be', 'excellent', 'The', 'weather', 'be', 'perfect', 'which', 'make', 'sit', 'outside', 'overlook', 'their', 'ground', 'an', 'absolute', 'pleasure', 'Our', 'waitress', 'be', 'excellent', 'and', 'our', 'food', 'arrive', 'quickly', 'on', 'the', 'semi-busy', 'Saturday', 'morning', 'It', 'look', 'like', 'the', 'place', 'fill', 'up', 'pretty', 'quickly', 'so', 'the', 'earlier', 'you', 'get', 'here', 'the', 'better', 'Do', 'yourself', 'a', 'favor', 'and', 'get', 'their', 'Bloody', 'Mary', 'It', 'be', 'phenomenal', 'and', 'simply', 'the', 'best', 'I', \"'ve\", 'ever', 'have', 'I', \"'m\", 'pretty', 'sure', 'they', 'only', 'use', 'ingredients', 'from', 'their', 'garden', 'and', 'blend', 'them', 'fresh', 'when', 'you', 'order', 'it', 'It', 'be', 'amaze', 'While', 'EVERYTHING', 'on', 'the', 'menu', 'look', 'excellent', 'I', 'have', 'the', 'white', 'truffle', 'scramble', 'egg', 'vegetable', 'skillet', 'and', 'it', 'be', 'tasty', 'and', 'delicious', 'It', 'come', 'with', '2', 'piece', 'of', 'their', 'griddle', 'bread', 'with', 'be', 'amaze', 'and', 'it', 'absolutely', 'make', 'the', 'meal', 'complete', 'It', 'be', 'the', 'best', 'toast', 'I', \"'ve\", 'ever', 'have', 'Anyway', 'I', 'ca', \"n't\", 'wait', 'to', 'go', 'back']\n"
     ]
    }
   ],
   "source": [
    "print([word.lemmatize(pos = 'v') for word in review.words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_into_lemmas(text):\n",
    "    \n",
    "    # pass every review/text through textblob\n",
    "    words = TextBlob(text).words\n",
    "    #return a list comprehension that lemmatizes every word \n",
    "    return [word.lemmatize(pos='v') for word in words]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = CountVectorizer(analyzer = split_into_lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3064x19368 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 243793 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_dtm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9354207436399217\n"
     ]
    }
   ],
   "source": [
    "#Create document-term matrices\n",
    "X_train_dtm = vect.fit_transform(X_train)\n",
    "X_test_dtm = vect.transform(X_test)\n",
    "\n",
    "\n",
    "#Intantiate a logistic regression model \n",
    "lr = LogisticRegression(max_iter = 10_000, solver='sag')\n",
    "\n",
    "#Fit our model with training data\n",
    "lr.fit(X_train_dtm, y_train)\n",
    "\n",
    "#Create a prediction\n",
    "y_pred = lr.predict(X_test_dtm)\n",
    "\n",
    "#measure accuracy score\n",
    "print(metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
